{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c499f4d-99e1-4057-8b1b-abb85458674b",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-14T11:08:20.119965Z",
     "iopub.status.busy": "2025-07-14T11:08:20.119620Z",
     "iopub.status.idle": "2025-07-14T11:08:35.309910Z",
     "shell.execute_reply": "2025-07-14T11:08:35.309129Z",
     "shell.execute_reply.started": "2025-07-14T11:08:20.119941Z"
    },
    "id": "7c499f4d-99e1-4057-8b1b-abb85458674b",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "dc59f186-3f83-4078-f803-58bbbda2638f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/sinzlab/neuralpredictors.git\n",
      "  Cloning https://github.com/sinzlab/neuralpredictors.git to /tmp/pip-req-build-_9j1vs2y\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/sinzlab/neuralpredictors.git /tmp/pip-req-build-_9j1vs2y\n",
      "  Resolved https://github.com/sinzlab/neuralpredictors.git to commit 2b420058b2c0c029842ba739829114ddfa0f8b50\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from neuralpredictors==0.3.0) (1.26.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from neuralpredictors==0.3.0) (2.1.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from neuralpredictors==0.3.0) (4.66.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neuralpredictors==0.3.0) (2.2.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from neuralpredictors==0.3.0) (3.10.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from neuralpredictors==0.3.0) (1.11.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->neuralpredictors==0.3.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->neuralpredictors==0.3.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neuralpredictors==0.3.0) (2023.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->neuralpredictors==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->neuralpredictors==0.3.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->neuralpredictors==0.3.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->neuralpredictors==0.3.0) (1.3.0)\n",
      "Building wheels for collected packages: neuralpredictors\n",
      "  Building wheel for neuralpredictors (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neuralpredictors: filename=neuralpredictors-0.3.0-py3-none-any.whl size=128528 sha256=b549cb12ea94aecaa77f54434cbf1c7eca2f5cc5d1293233dc41ddef71765cb5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l5h8ud9u/wheels/0e/f5/33/7ddb045df7300809f78d12190b4b6fb93ecd26243c1da24c88\n",
      "Successfully built neuralpredictors\n",
      "Installing collected packages: neuralpredictors\n",
      "Successfully installed neuralpredictors-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/sinzlab/neuralpredictors.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9170eb67-dd42-4360-8a09-b2cd7e0d4edb",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-14T11:08:35.311455Z",
     "iopub.status.busy": "2025-07-14T11:08:35.311206Z",
     "iopub.status.idle": "2025-07-14T11:08:40.428237Z",
     "shell.execute_reply": "2025-07-14T11:08:40.427435Z",
     "shell.execute_reply.started": "2025-07-14T11:08:35.311432Z"
    },
    "id": "9170eb67-dd42-4360-8a09-b2cd7e0d4edb",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.11.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (9.5.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2023.12.9)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.5.0)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525e4b42-131b-402b-a2ae-9af0da990362",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-14T11:08:40.434816Z",
     "iopub.status.busy": "2025-07-14T11:08:40.432661Z",
     "iopub.status.idle": "2025-07-14T11:08:45.501145Z",
     "shell.execute_reply": "2025-07-14T11:08:45.500410Z",
     "shell.execute_reply.started": "2025-07-14T11:08:40.434780Z"
    },
    "id": "525e4b42-131b-402b-a2ae-9af0da990362",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ac0bb6-4691-4efa-8721-faa169d70cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T11:08:45.502852Z",
     "iopub.status.busy": "2025-07-14T11:08:45.502643Z",
     "iopub.status.idle": "2025-07-14T11:08:46.700119Z",
     "shell.execute_reply": "2025-07-14T11:08:46.699201Z",
     "shell.execute_reply.started": "2025-07-14T11:08:45.502831Z"
    },
    "id": "d9ac0bb6-4691-4efa-8721-faa169d70cc8"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944dd0ee-be73-405e-983d-838aefa8f015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T11:08:46.701691Z",
     "iopub.status.busy": "2025-07-14T11:08:46.701325Z",
     "iopub.status.idle": "2025-07-14T11:08:54.033457Z",
     "shell.execute_reply": "2025-07-14T11:08:54.032843Z",
     "shell.execute_reply.started": "2025-07-14T11:08:46.701669Z"
    },
    "id": "944dd0ee-be73-405e-983d-838aefa8f015"
   },
   "outputs": [],
   "source": [
    "from utils import download_with_requests, set_background\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from digital_twin_library import SubsetSampler, corr, PoissonLoss\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from neuralpredictors.data.datasets import FileTreeDataset\n",
    "\n",
    "from neuralpredictors.data.transforms import (\n",
    "    ToTensor,\n",
    "    NeuroNormalizer,\n",
    "    ScaleInputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cd949-38f8-403a-9379-8663057dd8df",
   "metadata": {
    "id": "a20cd949-38f8-403a-9379-8663057dd8df"
   },
   "source": [
    "# Data\n",
    "\n",
    "Let's download some data from the sensorium competition 2022. You can check out all datasets [here](\"https://gin.g-node.org/cajal/Sensorium2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539c1cb4-77d7-4371-8005-601a076f4c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T11:09:58.155171Z",
     "iopub.status.busy": "2025-07-14T11:09:58.154654Z",
     "iopub.status.idle": "2025-07-14T11:10:19.241273Z",
     "shell.execute_reply": "2025-07-14T11:10:19.238751Z",
     "shell.execute_reply.started": "2025-07-14T11:09:58.155143Z"
    },
    "id": "539c1cb4-77d7-4371-8005-601a076f4c66"
   },
   "outputs": [],
   "source": [
    "!unzip -n -q /datasets/day01/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip -d sensorium_data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181971cb-7c47-4f59-bf73-a18746a9b1d2",
   "metadata": {
    "id": "181971cb-7c47-4f59-bf73-a18746a9b1d2"
   },
   "source": [
    "Let's look at the content of the data quickly. It contains images shown to a mouse, responses of cell in primary visual cortex (deconvolved Calcium activity), behavior of the animal (running and pupil dilation), and the location of the pupil center in an eye tracking video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4e379a-d1e9-4e89-840c-8ddaad0fb610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T11:12:27.757482Z",
     "iopub.status.busy": "2025-07-14T11:12:27.757192Z",
     "iopub.status.idle": "2025-07-14T11:12:28.017430Z",
     "shell.execute_reply": "2025-07-14T11:12:28.016683Z",
     "shell.execute_reply.started": "2025-07-14T11:12:27.757448Z"
    },
    "id": "ec4e379a-d1e9-4e89-840c-8ddaad0fb610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbehavior\u001b[0m/  \u001b[01;34mimages\u001b[0m/  \u001b[01;34mpupil_center\u001b[0m/  \u001b[01;34mresponses\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e96a1-b3e2-46d5-a546-1b984d0d82ad",
   "metadata": {
    "id": "081e96a1-b3e2-46d5-a546-1b984d0d82ad"
   },
   "outputs": [],
   "source": [
    "ls sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6/data/images/100*.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2eb5b-9e94-4fc9-a675-96a06849cbe4",
   "metadata": {
    "id": "7dd2eb5b-9e94-4fc9-a675-96a06849cbe4"
   },
   "outputs": [],
   "source": [
    "img = np.load('sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6/data/images/0.npy')\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4d8b0-7c2d-4a29-9129-185119f0d311",
   "metadata": {
    "id": "4bb4d8b0-7c2d-4a29-9129-185119f0d311"
   },
   "outputs": [],
   "source": [
    "responses = np.load('sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6/data/responses/0.npy')\n",
    "responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d7349-38a5-4ade-931e-ee58becf559c",
   "metadata": {
    "id": "4b7d7349-38a5-4ade-931e-ee58becf559c"
   },
   "source": [
    "The data contains gray scale images of size `144 x 256` and the corresponding responses of `8372` neurons in mouse primary visual cortex. For now, we'll ignore the behavior and the pupil and only use images and neuronal responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97d181-398f-4ddd-b265-b15c3c477dd4",
   "metadata": {
    "id": "0e97d181-398f-4ddd-b265-b15c3c477dd4"
   },
   "source": [
    "We can build a pytorch dataset for this data, using the the `FileTreeDataset` of `neuralpredictors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b9a73-de0a-4fec-a4e9-ba2d976b7a50",
   "metadata": {
    "id": "e52b9a73-de0a-4fec-a4e9-ba2d976b7a50"
   },
   "outputs": [],
   "source": [
    "from neuralpredictors.data.datasets import FileTreeDataset\n",
    "\n",
    "root_dir = 'sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6'\n",
    "dat = FileTreeDataset(root_dir, 'images', 'responses')\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043dad8-30a3-4137-aff7-609c370c714b",
   "metadata": {
    "id": "e043dad8-30a3-4137-aff7-609c370c714b"
   },
   "source": [
    "Together with a sampler, we can create a pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880e21a-b409-47fd-a28e-fb0556bd3e1a",
   "metadata": {
    "id": "5880e21a-b409-47fd-a28e-fb0556bd3e1a"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_sampler = SubsetSampler(dat.trial_info.tiers == 'train', shuffle=True)\n",
    "train_loader = DataLoader(dat, sampler=train_sampler, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef608172-332a-462d-92ba-76e2d6623a29",
   "metadata": {
    "id": "ef608172-332a-462d-92ba-76e2d6623a29"
   },
   "source": [
    "Now we can iterate through this data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508890d3-5079-41fd-bef9-2bdfeb2731c4",
   "metadata": {
    "id": "508890d3-5079-41fd-bef9-2bdfeb2731c4"
   },
   "outputs": [],
   "source": [
    "for images, responses in train_loader:\n",
    "    print(images.shape, responses.shape, type(images))\n",
    "    print(f\"Image mean+-std={images.mean()}+-{images.std()}\")\n",
    "    print(f\"Reponses mean+-std={responses.mean()}+-{responses.std()}\")\n",
    "    break # let's stop after the first iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004106f3-3437-4e51-9ba0-5ecc9b556f95",
   "metadata": {
    "id": "004106f3-3437-4e51-9ba0-5ecc9b556f95"
   },
   "source": [
    "For our purpose here, the images are still a bit big (training time), so we let's downscale them by a factor of 4. In addition, let's standardize the pixel values and scale the neuronal responses to have std=1. We can do that with the help of a few functions in `neuralpredictors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf1bb0-2c20-4703-9f31-f5b4db6e90ee",
   "metadata": {
    "id": "0adf1bb0-2c20-4703-9f31-f5b4db6e90ee"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "root_dir = 'sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6'\n",
    "dat = FileTreeDataset(root_dir, 'images', 'responses')\n",
    "\n",
    "transforms = [NeuroNormalizer(dat), ScaleInputs(scale=0.25), ToTensor(torch.cuda.is_available())]\n",
    "dat.transforms.extend(transforms)\n",
    "\n",
    "\n",
    "train_sampler = SubsetSampler(dat.trial_info.tiers == 'train', shuffle=True)\n",
    "train_loader = DataLoader(dat, sampler=train_sampler, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26d569-82b6-4489-b042-6d7570611694",
   "metadata": {
    "id": "2c26d569-82b6-4489-b042-6d7570611694"
   },
   "outputs": [],
   "source": [
    "for images, responses in train_loader:\n",
    "    print(images.shape, responses.shape, type(images))\n",
    "    print(f\"Image mean+-std={images.mean()}+-{images.std()}\")\n",
    "    print(f\"Reponses mean+-std={responses.mean()}+-{responses.std()}\")\n",
    "    break # let's stop after the first iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e0f8b-b727-4def-a0d1-32bce098950c",
   "metadata": {
    "id": "f88e0f8b-b727-4def-a0d1-32bce098950c"
   },
   "source": [
    "# Train our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be56f23-c4f2-4a3e-a13a-43d9c1a3982c",
   "metadata": {
    "id": "9be56f23-c4f2-4a3e-a13a-43d9c1a3982c"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root_dir = 'sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6'\n",
    "dat = FileTreeDataset(root_dir, 'images', 'responses')\n",
    "\n",
    "transforms = [ScaleInputs(scale=0.125), ToTensor(torch.cuda.is_available())]\n",
    "dat.transforms.extend(transforms)\n",
    "\n",
    "train_sampler = SubsetSampler(dat.trial_info.tiers == 'train', shuffle=True)\n",
    "test_sampler = SubsetSampler(dat.trial_info.tiers == 'test', shuffle=False)\n",
    "val_sampler = SubsetSampler(dat.trial_info.tiers == 'validation', shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(dat, sampler=train_sampler, batch_size=64)\n",
    "val_loader = DataLoader(dat, sampler=val_sampler, batch_size=64)\n",
    "test_loader = DataLoader(dat, sampler=test_sampler, batch_size=64)\n",
    "\n",
    "print(f\"The training device is: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e53da-9e0c-4f68-963d-bb0187fe1cb5",
   "metadata": {
    "id": "283e53da-9e0c-4f68-963d-bb0187fe1cb5"
   },
   "source": [
    "<div class=\"task\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b47cfc-b6fd-4c30-9ae5-61101119127f",
   "metadata": {
    "id": "37b47cfc-b6fd-4c30-9ae5-61101119127f"
   },
   "source": [
    "## First model\n",
    "\n",
    "Now we are ready to build our first model. Please go through the tasks in order and see how/whether the performance of the model improves\n",
    "\n",
    "1. Below you find a simple linear model. Write code to train the model on the training data.\n",
    "    * What kind of loss could you use?\n",
    "    * Monitor the performance (correlation) on the validation all 5 epochs (epoch=full sweep through the training data).\n",
    "1. You'll find that the model is slow and does not train very well. Now try the following improvements and see how they increase the prediction performance.\n",
    "    * Decrease the image resolution to `18 x 32`.\n",
    "    * Add a `nn.BatchNorm1d` layer before the linear layer (make sure to put the model into `train` and `eval` mode respectively).\n",
    "    * Add `weight_decay=1e-4` to the optimizer.\n",
    "    * Add a rectifying nonlinearity `ReLU`, `Softplus` or `ELU + 1`.\n",
    "    * Change the loss to `digital_twin_library.PoissonLoss`.\n",
    "1. For the best model, produce plots that\n",
    "    * compare the performance (correlation) between validation and train for all neurons.\n",
    "    * plot linear filters (as grayscale images) of the best `n=12` neurons. What would you expect to see?\n",
    "1. Think about the following questions\n",
    "    * How many parameters does the model have?\n",
    "    * Is there any benefit from having many neurons?\n",
    "    * **Advanced**: The Poisson loss strictly a \"correct\" loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87582796-1f01-458c-a134-62330e1e6301",
   "metadata": {
    "id": "87582796-1f01-458c-a134-62330e1e6301"
   },
   "outputs": [],
   "source": [
    "class GLM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten the images\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b994a-7d9d-44a2-afc4-66015e3f4231",
   "metadata": {
    "id": "781b994a-7d9d-44a2-afc4-66015e3f4231"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042b28e-7a74-4e1f-808a-ec5647325df3",
   "metadata": {
    "id": "9042b28e-7a74-4e1f-808a-ec5647325df3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036541d0-6b3c-4594-a9c7-38f5ba9d41c3",
   "metadata": {
    "id": "036541d0-6b3c-4594-a9c7-38f5ba9d41c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acd65a-bf46-4d17-aa07-f7fc84f41986",
   "metadata": {
    "id": "29acd65a-bf46-4d17-aa07-f7fc84f41986"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3edb57e-00dc-4ae3-925f-4b4e8e36e86e",
   "metadata": {
    "id": "b3edb57e-00dc-4ae3-925f-4b4e8e36e86e"
   },
   "source": [
    "# Improving the model\n",
    "\n",
    "Let's improve the model. First, let's use resolution `36 x 64` from now on. Also, let's standardize the images to `mean+-std=0+-1` and the neuronal responses to `std=1` (we keep them positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07636d83-7ae0-4704-9a19-b2b4b6a32d33",
   "metadata": {
    "id": "07636d83-7ae0-4704-9a19-b2b4b6a32d33"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root_dir = 'sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6'\n",
    "dat = FileTreeDataset(root_dir, 'images', 'responses')\n",
    "\n",
    "transforms = [NeuroNormalizer(dat), ScaleInputs(scale=0.25), ToTensor(torch.cuda.is_available())]\n",
    "dat.transforms.extend(transforms)\n",
    "\n",
    "train_sampler = SubsetSampler(dat.trial_info.tiers == 'train', shuffle=True)\n",
    "test_sampler = SubsetSampler(dat.trial_info.tiers == 'test', shuffle=False)\n",
    "val_sampler = SubsetSampler(dat.trial_info.tiers == 'validation', shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(dat, sampler=train_sampler, batch_size=64)\n",
    "val_loader = DataLoader(dat, sampler=val_sampler, batch_size=64)\n",
    "test_loader = DataLoader(dat, sampler=test_sampler, batch_size=64)\n",
    "\n",
    "print(f\"The training device is: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04226fdf-423d-4f6b-baac-441e5410f6ad",
   "metadata": {
    "id": "04226fdf-423d-4f6b-baac-441e5410f6ad"
   },
   "source": [
    "Let's now try to make our model better. You can try the following options\n",
    "\n",
    "1. Make the model convolutional:\n",
    "    * Instead of a linear layer, use a convolutional network with layers `(Conv2D, Batchnorm2D, Nonlinearity)`.\n",
    "    * Use a final linear layer + rectifying nonlinearity to map the output of the network to the neurons.\n",
    "    * A good start is 3 convolutional layers + final linear layer.\n",
    "    * How many parameters does your model have? Is there a benefit of having many neurons now?\n",
    "2. Reduce parameters\n",
    "   * Replace the convolution by depthwise separable convolutions\n",
    "   * Factorize the readout as in [Klindt et al. 2017](https://arxiv.org/abs/1711.02653).\n",
    "   * **Advanced**: Implement a Gaussian Readout as in [Lurz et al. 2021](https://openreview.net/pdf?id=Tp7kI90Htd).\n",
    "3. Improve the training routine\n",
    "   * Use early stopping: Monitor the validation correlation. If it has not gone up for `patience` epochs, set the model to the best model so far and stop training.\n",
    "   * Use learning rate schedules: Instead of stopping training, reduce the learning rate once or twice (e.g. with [ReduceLROnPlateau](https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html) ).\n",
    "\n",
    "**Challenge**:\n",
    "Let's make this a competition. Try to get the best model you can with the above (or other) tricks. We'll maintain a leader board at the front. Report your best validation correlation to us and we mark it down. Obviously, you're not allowed to train on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1a673-216c-45c6-a697-1afbb63c9e2e",
   "metadata": {
    "id": "4ca1a673-216c-45c6-a697-1afbb63c9e2e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee6201-e4a7-40db-9a2e-404e0ab2e747",
   "metadata": {
    "id": "afee6201-e4a7-40db-9a2e-404e0ab2e747"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeab198-a556-4dd7-8ce1-5ed20dfae6b3",
   "metadata": {
    "id": "feeab198-a556-4dd7-8ce1-5ed20dfae6b3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87087ab1-0a88-46e0-8009-179cb17efabc",
   "metadata": {
    "id": "87087ab1-0a88-46e0-8009-179cb17efabc"
   },
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4314e6-0549-434e-bcdc-914b643ff143",
   "metadata": {
    "id": "fd4314e6-0549-434e-bcdc-914b643ff143"
   },
   "source": [
    "Now that we have a well trained model, we can start analyzing it. However, unlike before, we cannot simply plot linear filters. However, we can run classical analyses (and more) on the model. In the following block we'll look at receptive fields.\n",
    "\n",
    "1. Run spike triggered averages on the model. I.e. present the model with Gaussian white noise (what previous modelling choice was important for that?) and record the responses. Then, for each if the best 12 neurons, compute the weighted mean of the noise frames using the predicted response of the neuron as weights. Plot the resulting images. What can you observe?\n",
    "\n",
    "2. What happens if you do 1. with natural images. How can you explain the difference?\n",
    "\n",
    "3. In stead of using STA, send the zero image through the model and compute the gradient with respect to the image. This can be done via\n",
    "\n",
    "    ```python\n",
    "    rfs = []\n",
    "    for i in best[:12]:\n",
    "        x = torch.zeros(1, 1, 36, 64).to(device)\n",
    "        x.requires_grad = True\n",
    "        r = model_m(x)\n",
    "        r[0, i].backward()\n",
    "        rfs.append(x.grad.cpu().numpy().squeeze())\n",
    "    ```\n",
    "    \n",
    "    Plot the resulting images, too. What can you observe? Can you explain why you get this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c26f9-d548-4ae6-8e85-217f96320a45",
   "metadata": {
    "id": "e98c26f9-d548-4ae6-8e85-217f96320a45"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250a090-eb44-498d-b92e-e97a3731e2c6",
   "metadata": {
    "id": "b250a090-eb44-498d-b92e-e97a3731e2c6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e469d-2796-427a-8c64-7327224818f1",
   "metadata": {
    "id": "fc8e469d-2796-427a-8c64-7327224818f1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629dc4b-5a12-4dd7-9053-ed308f2f9beb",
   "metadata": {
    "id": "b629dc4b-5a12-4dd7-9053-ed308f2f9beb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00286f13-8bce-480b-a31d-15db691a7d8a",
   "metadata": {
    "id": "00286f13-8bce-480b-a31d-15db691a7d8a"
   },
   "source": [
    "# Optional: How good is good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bf31e-f5aa-478d-ad54-54b1c93d07a4",
   "metadata": {
    "id": "d37bf31e-f5aa-478d-ad54-54b1c93d07a4"
   },
   "source": [
    "The best models so far, probably reached about `0.3` in correlation. Since correlation goes up until `1` this might not seem like a good result. However, neuronal responses are noisy. So even if we had the best predictor, we would not reach `1`. Thus, the question arises what's the best we could do.\n",
    "\n",
    "For many loss functions, the best predictor is the conditional mean, i.e. the average response to the same image over many repetitions. We can estimate this response from repeated presentations of the same stimulus. The dataset actually contains these kind of repetitions. However, for that we need a new sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b399c-2577-4beb-b403-92e64b689778",
   "metadata": {
    "id": "cd5b399c-2577-4beb-b403-92e64b689778"
   },
   "outputs": [],
   "source": [
    " # this sampler groups all responses to the same image in one batch\n",
    "from neuralpredictors.data.samplers import RepeatsBatchSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66607c2f-ebd5-4a57-b20a-06d140a81a56",
   "metadata": {
    "id": "66607c2f-ebd5-4a57-b20a-06d140a81a56"
   },
   "outputs": [],
   "source": [
    "root_dir = 'sensorium_data/static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6'\n",
    "dat = FileTreeDataset(root_dir, 'images', 'responses')\n",
    "\n",
    "transforms = [NeuroNormalizer(dat), ScaleInputs(scale=0.25), ToTensor(torch.cuda.is_available())]\n",
    "dat.transforms.extend(transforms)\n",
    "\n",
    "test_sampler = RepeatsBatchSampler(dat.trial_info.frame_image_id, np.where(dat.trial_info.tiers == 'test')[0])\n",
    "\n",
    "test_loader = DataLoader(dat, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080c756-8918-4aec-bb01-5dc70049333c",
   "metadata": {
    "id": "d080c756-8918-4aec-bb01-5dc70049333c"
   },
   "source": [
    "Test that you actually get the same image in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2c1fb-03c3-4965-a450-3bce2ed76886",
   "metadata": {
    "id": "4ff2c1fb-03c3-4965-a450-3bce2ed76886"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02da3a3-cd8b-48d2-8c6b-b9f007b9effc",
   "metadata": {
    "id": "b02da3a3-cd8b-48d2-8c6b-b9f007b9effc"
   },
   "source": [
    "Now use this dataset to compute an \"oracle\" predictor. Consider the response $y_{ij}$ of a neuron to the $i$th image shown the $j$th time. To compute the oracle predictor for this trial, we average all responses of that neuron to this image over all **but** the $j$th trial\n",
    "\n",
    "$$\\hat y_{ij} = \\frac{1}{n-1} \\sum_{k\\not=j} y_{ik}$$\n",
    "\n",
    "where $n$ is the number of repeats. Then use this predictor and correlate it with all $y_{ij}$ as if the oracle were model predictions. Compare that to the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3ded4-33c1-46df-a8e7-71a51188e135",
   "metadata": {
    "id": "39c3ded4-33c1-46df-a8e7-71a51188e135"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411dc60-21c5-4444-b223-39fe6c086805",
   "metadata": {
    "id": "f411dc60-21c5-4444-b223-39fe6c086805"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efe40d-10e6-4a53-ae53-d55f9a80c465",
   "metadata": {
    "id": "a6efe40d-10e6-4a53-ae53-d55f9a80c465"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
